{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3D_ellipse(n, r1 = 1, r2 = 0.5):\n",
    "    t = torch.linspace(0, 2 * torch.pi, n+1)\n",
    "    x = (r1 * torch.cos(t)).view(-1, 1)[:-1]\n",
    "    y = (r2 * torch.sin(t)).view(-1, 1)[:-1]\n",
    "    z = (0 * torch.sin(t)).view(-1, 1)[:-1]\n",
    "    return torch.cat((x, y, z), dim=1).unsqueeze(0)\n",
    "\n",
    "\n",
    "def generate_ellipse(n, r1 = 1, r2 = 0.5):\n",
    "    t = torch.linspace(0, 2*torch.pi, n+1)\n",
    "    x = (r1 * torch.cos(t)).view(-1, 1)[:-1]\n",
    "    y = (r2 * torch.sin(t)).view(-1, 1)[:-1]\n",
    "    return torch.cat((x, y), dim=1).unsqueeze(0)\n",
    "\n",
    "def generate_data(n, sign = \"+\"):\n",
    "\n",
    "    el = generate_ellipse(n, r1 = 0.5, r2 = 1)\n",
    "    e2 = generate_ellipse(n, r1 = 1, r2 = 0.5)\n",
    "    return torch.concat((el, e2), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H_theta(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(H_theta, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "\n",
    "def generate_NN_latent_functions(num_samples, xdim=1, zdim=2, bias=0):\n",
    "    class NN(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super(NN, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, 100)\n",
    "            self.fc2 = nn.Linear(100, 50)\n",
    "            self.fc3 = nn.Linear(50, 50)\n",
    "            self.fc4 = nn.Linear(50, output_dim)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = torch.relu(self.fc3(x))\n",
    "            x = self.fc4(x)*5\n",
    "            return x\n",
    "\n",
    "    #  weight initialization function\n",
    "    def weights_init_normal(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight, gain = 0.5)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, bias)\n",
    "\n",
    "    #  neural networks\n",
    "    networks = []\n",
    "    for _ in range(num_samples):\n",
    "        net = NN(xdim, zdim)\n",
    "        net.apply(weights_init_normal)\n",
    "        networks.append(net)\n",
    "    return networks\n",
    "\n",
    "def find_nns(Y, G):\n",
    "    #Y: [1, 1024, 3]\n",
    "    #G: [20, 1024, 3]\n",
    "\n",
    "    distances = torch.sum(((Y - G) ** 2), dim = 2).mean(dim = 1)\n",
    "    _, min_idx = torch.min(distances, dim=0)\n",
    "    return min_idx.item()\n",
    "\n",
    "def f_loss(Y, G):\n",
    "    weighted_diffs = (G - Y)**2\n",
    "    diffs = torch.sum(weighted_diffs, dim=2)\n",
    "    total_loss = diffs.mean(dim=1).mean(dim=0)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: case_bias_0_zero_True_concat_True_multiply_True_2D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3540/5000 [00:15<00:06, 233.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 188\u001b[0m\n\u001b[0;32m    185\u001b[0m     output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Call the train function with the current settings\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m H_t \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiply_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiply_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_zero\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m test(zdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, xdim\u001b[38;5;241m=\u001b[39mxdim, output_dim\u001b[38;5;241m=\u001b[39moutput_dim, H_t\u001b[38;5;241m=\u001b[39mH_t, concat\u001b[38;5;241m=\u001b[39mconcat)\n",
      "Cell \u001b[1;32mIn[4], line 50\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(concat, multiply_output, zdim, xdim, output_dim, bias, include_zero, epochs, num_points, lr, staleness, num_Z_samples)\u001b[0m\n\u001b[0;32m     48\u001b[0m Zs \u001b[38;5;241m=\u001b[39m generate_NN_latent_functions(num_samples\u001b[38;5;241m=\u001b[39mnum_Z_samples, xdim\u001b[38;5;241m=\u001b[39mxdim, zdim\u001b[38;5;241m=\u001b[39mzdim, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Zs):\n\u001b[1;32m---> 50\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multiply_output:\n",
      "File \u001b[1;32mc:\\Users\\rajsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rajsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rajsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\rajsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "def train(concat=True, multiply_output=False, zdim=5, xdim=1, output_dim=2, bias=0, include_zero=False, epochs=100, num_points=1600, lr=0.001, staleness=15, num_Z_samples=30):\n",
    "\n",
    "    folder_name = f\"case_bias_{bias}_zero_{include_zero}_concat_{concat}_multiply_{multiply_output}_{output_dim}D\"\n",
    "    output_dir = os.path.join(\"cases_results\", folder_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f'Training: {folder_name}')\n",
    "\n",
    "    if concat:\n",
    "        input_dim = zdim + xdim\n",
    "    else:\n",
    "        input_dim = zdim\n",
    "\n",
    "    H_t = H_theta(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "    optimizer = optim.Adam(H_t.parameters(), lr=lr)\n",
    "    losses = []\n",
    "\n",
    "    if xdim == 1:\n",
    "        if include_zero:\n",
    "            x = torch.linspace(-1.0, 2.0, num_points).to(device).unsqueeze(1)\n",
    "        else:\n",
    "            x = torch.linspace(1.0, 2.0, num_points).to(device).unsqueeze(1)\n",
    "    else:\n",
    "        if include_zero:\n",
    "            x1 = torch.linspace(-1.0, 2.0, 40)\n",
    "            x2 = torch.linspace(-1.0, 2.0, 40)\n",
    "        else:\n",
    "            x1 = torch.linspace(1.0, 2.0, 40)\n",
    "            x2 = torch.linspace(1.0, 2.0, 40)\n",
    "        grid_x1, grid_x2 = torch.meshgrid((x1, x2), indexing='ij')\n",
    "        x = torch.stack((grid_x1, grid_x2), dim=-1).reshape(-1, 2).to(device)\n",
    "\n",
    "    # Generate data\n",
    "    data = generate_data(num_points).to(device)\n",
    "    if output_dim == 3:\n",
    "        points1 = generate_3D_ellipse(num_points).to(device)\n",
    "        points2 = generate_3D_ellipse(num_points, r1=0.5, r2=1.0).to(device)\n",
    "        data = torch.concat((points1, points2), dim=0).to(device)\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        with torch.no_grad():\n",
    "            if e % staleness == 0:\n",
    "                H_t.eval()\n",
    "                Zxs = torch.empty((num_Z_samples, num_points, input_dim)).to(device)\n",
    "                Zs = generate_NN_latent_functions(num_samples=num_Z_samples, xdim=xdim, zdim=zdim, bias=bias)\n",
    "                for i, model in enumerate(Zs):\n",
    "                    model = model.to(device)\n",
    "                    Z = 0\n",
    "                    if multiply_output:\n",
    "                        Z = model(x) * 50  # Multiply output\n",
    "                    else:\n",
    "                        Z = model(x)\n",
    "                    if concat:\n",
    "                        Zxs[i] = torch.cat((Z, x), dim=1).to(device) #Concat\n",
    "                    else:\n",
    "                        Zxs[i] = (Z).to(device)\n",
    "                \n",
    "                generated = H_t(Zxs).to(device)\n",
    "                imle_nns = torch.tensor([find_nns(d, generated) for d in data], dtype=torch.long)\n",
    "                imle_transformed_points = Zxs[imle_nns]\n",
    "                H_t.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outs = H_t(imle_transformed_points)\n",
    "        loss = f_loss(data, outs)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ((e-1) % int(epochs/2) == 0 or e == epochs-1) and e - 1 != 0:\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            outs_disp = outs.to(device='cpu').detach().numpy()\n",
    "            points_disp = data.to(device='cpu').detach().numpy()\n",
    "            for i in range(1):\n",
    "                plt.plot(points_disp[0, :, 0], points_disp[0, :, 1], c='red', alpha=0.5)\n",
    "                plt.plot(points_disp[1, :, 0], points_disp[1, :, 1], c='blue', alpha=0.5)\n",
    "                plt.scatter(outs_disp[0, :, 0], outs_disp[0, :, 1], s=1.5, c='red', marker='*')\n",
    "                plt.scatter(outs_disp[1, :, 0], outs_disp[1, :, 1], s=1.5, c='blue', marker='*')\n",
    "            plt.title(f'Output at epoch {e}')\n",
    "            plt.savefig(os.path.join(output_dir, f'output_epoch_{e}.png'))\n",
    "            plt.close()\n",
    "\n",
    "    # Plot and save the loss curve\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.savefig(os.path.join(output_dir, 'loss_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(losses.pop())\n",
    "\n",
    "    return H_t\n",
    "\n",
    "\n",
    "def test(zdim, xdim, output_dim, H_t, include_zero=False, num_points=1600, concat = False, multiply_output = False):\n",
    "    # Create the output directory for test results\n",
    "    folder_name = f\"case_bias_{bias}_zero_{include_zero}_concat_{concat}_multiply_{multiply_output}_{output_dim}D\"\n",
    "    output_dir = os.path.join(\"cases_results\", folder_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if concat:\n",
    "        input_dim = zdim + xdim\n",
    "    else:\n",
    "        input_dim = zdim\n",
    "    # Generate x based on xdim\n",
    "    if xdim == 1:\n",
    "        if include_zero:\n",
    "            x = torch.linspace(-1.0, 2.0, num_points).to(device).unsqueeze(1)\n",
    "        else:\n",
    "            x = torch.linspace(1.0, 2.0, num_points).to(device).unsqueeze(1)\n",
    "    else:\n",
    "        if include_zero:\n",
    "            x1 = torch.linspace(-1.0, 2.0, 40)\n",
    "            x2 = torch.linspace(-1.0, 2.0, 40)\n",
    "        else:\n",
    "            x1 = torch.linspace(1.0, 2.0, 40)\n",
    "            x2 = torch.linspace(1.0, 2.0, 40)\n",
    "        grid_x1, grid_x2 = torch.meshgrid((x1, x2), indexing='ij')\n",
    "        x = torch.stack((grid_x1, grid_x2), dim=-1).reshape(-1, 2).to(device)\n",
    "\n",
    "    # Generate data\n",
    "    data = generate_data(num_points).to(device)\n",
    "    if output_dim == 3:\n",
    "        points1 = generate_3D_ellipse(num_points).to(device)\n",
    "        points2 = generate_3D_ellipse(num_points, r1=0.5, r2=1.0).to(device)\n",
    "        data = torch.concat((points1, points2), dim=0).to(device)\n",
    "\n",
    "    # Generate test points\n",
    "    num_samples = 10\n",
    "    Zxs = torch.empty((num_samples, num_points, input_dim)).to(device)\n",
    "    data = data.to(\"cpu\").detach().numpy()\n",
    "    Zs = generate_NN_latent_functions(num_samples=num_samples, xdim=xdim, zdim=zdim)\n",
    "\n",
    "    for i, model in enumerate(Zs):\n",
    "        Z = 0\n",
    "        model = model.to(device)\n",
    "        if multiply_output:\n",
    "            Z = model(x) * 50  # Multiply output\n",
    "        else:\n",
    "            Z = model(x)\n",
    "        if concat:\n",
    "            Zxs[i] = torch.cat((Z, x), dim=1).to(device)\n",
    "        else:\n",
    "            Zxs[i] = (Z).to(device)\n",
    "        \n",
    "\n",
    "\n",
    "    generated = H_t(Zxs).to(\"cpu\").detach().numpy()\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for i in range(generated.shape[0]):\n",
    "        \n",
    "        for c in data:\n",
    "            plt.plot(c[:, 0], c[:, 1], alpha=0.5)\n",
    "        plt.scatter(generated[i, :, 0], generated[i, :, 1], alpha=1, s=1)\n",
    "\n",
    "    plt.xlabel(\"X-axis\")\n",
    "    plt.ylabel(\"Y-axis\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"Generated_Results.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Define the parameter options\n",
    "bias_options = [0, 1]\n",
    "include_zero_options = [True, False]\n",
    "concat_options = [True, False]\n",
    "multiply_output_options = [True, False]\n",
    "ellipse_options = ['2D', '3D']\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(itertools.product(bias_options, include_zero_options, concat_options, multiply_output_options, ellipse_options))\n",
    "\n",
    "# Iterate through each combination\n",
    "for combination in combinations:\n",
    "    bias, include_zero, concat, multiply_output, ellipse = combination\n",
    "    \n",
    "    # Set xdim and output_dim based on 2D/3D choice\n",
    "    if ellipse == '2D':\n",
    "        xdim = 1\n",
    "        output_dim = 2\n",
    "    else:\n",
    "        xdim = 2\n",
    "        output_dim = 3\n",
    "\n",
    "    # Call the train function with the current settings\n",
    "    H_t = train(concat=concat, multiply_output=multiply_output, zdim=15, xdim=xdim, output_dim=output_dim, bias=bias, include_zero=include_zero, epochs=5000)\n",
    "    test(zdim=15, xdim=xdim, output_dim=output_dim, H_t=H_t, concat=concat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
