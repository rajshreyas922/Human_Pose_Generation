{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3D_ellipse(n, r1 = 1, r2 = 0.5):\n",
    "    t = torch.linspace(-3*torch.pi, 3 * torch.pi, n)\n",
    "    x = (r1 * torch.cos(t)).view(-1, 1)\n",
    "    y = (r2 * torch.sin(t)).view(-1, 1)\n",
    "    z = (0 * torch.sin(t)).view(-1, 1)\n",
    "    return torch.cat((x, y, z), dim=1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_3D_ellipse(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_NN_latent_functions(num_samples, xdim=1, zdim=2, lambda_value=1):\n",
    "    class NN(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super(NN, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, 100)\n",
    "            self.fc2 = nn.Linear(100, 100)\n",
    "            self.fc3 = nn.Linear(100, 50)\n",
    "            self.fc4 = nn.Linear(50, output_dim)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = (self.fc1(x))\n",
    "            x = (self.fc2(x))\n",
    "            x = (self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            return x\n",
    "\n",
    "    #  weight initialization function\n",
    "    def weights_init_normal(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=1.0)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    #  neural networks\n",
    "    networks = []\n",
    "    for _ in range(num_samples):\n",
    "        net = NN(xdim, zdim)\n",
    "        net.apply(weights_init_normal)\n",
    "        networks.append(net)\n",
    "    return networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNs_test = generate_NN_latent_functions(2)\n",
    "model = NNs_test[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.4248],\n",
      "        [-5.6549],\n",
      "        [-1.8850],\n",
      "        [ 1.8850],\n",
      "        [ 5.6549],\n",
      "        [ 9.4248]], device='cuda:0')\n",
      "tensor([[-7546.5513, -4974.4814],\n",
      "        [-4527.9331, -2984.6890],\n",
      "        [-1509.3104,  -994.8967],\n",
      "        [ 1509.3104,   994.8967],\n",
      "        [ 4527.9331,  2984.6890],\n",
      "        [ 7546.5513,  4974.4814]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[-5151.4287, -2511.2825],\n",
      "        [-3090.8569, -1506.7711],\n",
      "        [-1030.2854,  -502.2565],\n",
      "        [ 1030.2854,   502.2565],\n",
      "        [ 3090.8569,  1506.7711],\n",
      "        [ 5151.4287,  2511.2825]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t = torch.linspace(-3*torch.pi, 3*torch.pi, 6).to(device).unsqueeze(1)\n",
    "print(t)\n",
    "print(NNs_test[0].to(device)(t))\n",
    "print(NNs_test[1].to(device)(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H_theta(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(H_theta, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_t = H_theta(input_dim=2, output_dim=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
